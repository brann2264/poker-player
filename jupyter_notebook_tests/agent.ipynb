{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque, namedtuple\n",
    "import random, datetime, os, copy\n",
    "\n",
    "import gymnasium as gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player():\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        # Action Variables\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.net = PokerDQN(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device=self.device)\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "\n",
    "        self.curr_step = 0\n",
    "        self.save_every = 5e5\n",
    "\n",
    "        # Memory Variables\n",
    "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(max_size=100000))\n",
    "        self.batch_size = 32\n",
    "\n",
    "        # Learn Variables\n",
    "        self.gamma = 0.9\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=1e-4)\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "        self.sync_steps = 3\n",
    "\n",
    "    def act(self, state):\n",
    "        #Explore\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_id = np.random.randint(self.action_dim)\n",
    "        #Exploit\n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            state = torch.tensor(state)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_id = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        self.exploration_rate += self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate, self.exploration_rate_min)\n",
    "        self.curr_step += 1\n",
    "\n",
    "        return action_id\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        # Adds current info to memory\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "        state = torch.tensor(state)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        action = torch.tensor([action])\n",
    "        reward = torch.tensor([reward])\n",
    "        done = torch.tensor([done])\n",
    "\n",
    "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
    "    \n",
    "    def recall(self):\n",
    "        # Retrieves the last batch of memories from the memory\n",
    "        memory_batch = self.memory.sample(batch_size=self.batch_size)\n",
    "        state, next_state, action, reward, done = (memory_batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "\n",
    "        return state, next_state, action, reward, done\n",
    "    \n",
    "    def Q_current(self, state, action):\n",
    "        return self.net(state, model=\"current\")[np.arange(0, self.batch_size), action]\n",
    "\n",
    "    def Q_target(self, reward, next_state, done):\n",
    "        # Current reward + Q of the next state\n",
    "        with torch.no_grad():\n",
    "            current_Q_pred = self.net(next_state, model=\"current\")\n",
    "            best_action = torch.argmax(current_Q_pred, axis=1)\n",
    "            next_Q = self.net(next_state, model=\"target\")[np.arange(0, self.batch_size), best_action]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "    \n",
    "    def optim_Q_current(self, current_est, target):\n",
    "        loss = self.loss_fn(current_est, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.bacmward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def sync_target(self):\n",
    "        self.net.target.load_state_dict(self.net.current.state_dict())\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_steps == 0:\n",
    "            self.sync_target\n",
    "    \n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "        td_current_est = self.Q_current(state, action)\n",
    "        td_target = self.Q_target(reward, next_state, done)\n",
    "        loss = self.optim_Q_current(td_current_est, td_target)\n",
    "        return loss\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save(self.net.state_dict(), self.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PokerDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        super().__init__()\n",
    "        self.current = nn.Sequential(nn.Linear(num_states, 64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(64, 64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(64, num_actions))\n",
    "        self.target = copy.deepcopy(self.current)\n",
    "    \n",
    "    def forward(self, x, model):\n",
    "        if model == \"current\":\n",
    "            return self.current(x)\n",
    "        if model == \"target\":\n",
    "            return self.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PokerTable():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 40\n",
    "\n",
    "player = Player(state_dim=8, action_dim=6, save_dir=None)\n",
    "env = PokerTable()\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = player.act(state)\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "        player.cache(state, next_state, action, reward, done)\n",
    "        loss = player.learn()\n",
    "        state = next_state\n",
    "\n",
    "        if done or info['flag_get']:\n",
    "            print(f'Episode {episode} | Loss: {loss}')\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 ('pokerRL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1708500d269b63431ea89f71f050295cfadbbf912cc1a386cef19049b13fc9c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
